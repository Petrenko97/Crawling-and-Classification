{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IhAjgg8oVc5"
   },
   "source": [
    "# PDS A2\n",
    "You will be shared ten files from coders who annotated verses from the 1st book of Iliad (no names shared). In each file there are the following columns: polarity (the sentiment the reader felt while reading the verse), emotions, hero (Homer narrating or a hero talking). The goal of this assignment is to build Machine Learning for automated sentiment annotation.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploring the data.\n",
    "  * Address the missing values (and any outliers). \n",
    "  * Measure inter-annotator agreement in the *polarity* and *hero* columns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preprocessing.\n",
    "  * Map the `polarity` (positive, negative, no emotion) to scores (respectively: 1, -1, 0). \n",
    "  * Perform a data exploratory analysis for `polarity` by visualising the class balance per annotator, and the variance per verse and per annotator. Combine `polarity` with `emotions` and `hero` to explore the data further (for example, one could study the aggregated sentiment-score per hero/narrator or the emotion distribution across the sentiment scores). Note that all figures should comply with the ten rules of visualisation that were taught in class. \n",
    "  * Suggest three findings (max: 50 words each) that will be based on the visualisations of your exploratory analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Automated annotation.\n",
    " * Build baselines (at least one based on random guesses) and regressors (at least three sklearn-based) that will yield a score (from -1 to 1) estimating the reader's sentiment for an unseen verse. \n",
    " * Evaluate your models using mean absolute error (MAE) and mean square error (MSE). Turn the gold and predicted scores to classes (-1, 0, 1) and evaluate also using *proper* classification evaluation metrics.  \n",
    " * Diagnose and analyse any under/over fitting. \n",
    " * Announce a winner based on your evaluation and apply it in order to predict a label (not score) per verse on the 24th Iliad book that is given. Submit your predictions as a compressed CSV with the following title: `IB24.your-student-ID-number.csv.gz`, where `your-student-ID-number` will hold your student ID number). The submitted dataframe should comprise the verses in one column (exactly as the original), but it should also comprise another column to hold the aligned predictions. \n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Scraping and silver labeling.\n",
    " * Scrape all the books of Iliad, by using this [translation from Project Gutenberg](https://www.gutenberg.org/cache/epub/36248/pg36248-images.html).\n",
    " * Use your best performing sentiment classifier from (3) to label the verses of all the 24 crawled books (silver labeling).  \n",
    " * Visualise the sentiment series resulted from the silver labels of all the books. \n",
    " * Evaluate your model's predictions for the 1st scraped book with the respective gold annotations (of the same book from T3, yet with a different translation), which you used to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BONUS:  \n",
    "\n",
    "Try to improve your best performing model, in order to better generalise. Your labels of the 24th will be compared against those of a quality annotator to evaluate your model. The top five scores will receive a bonus of 10%. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus coding..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Submission: \n",
    "* A zipped folder (only ZIP, not RAR), with your student ID number as name (e.g., f12345.zip), which will include the following files: \n",
    "  * The notebook with the tasks and the solutions, named as: `your-student-ID-number.ipynb`.\n",
    "  * The dataframe with the ground truth annotations from the second task (T2) named as: `IB1.your-student-ID-number.csv.gz`\n",
    "  * The predictions for the 24th book (from T3), named as: `IB24.your-student-ID-number.csv.gz`\n",
    "  * The dataframe with the silver annotations of all the scrapped books (from T4), using a long format and named as: `iliad_from_gutenberg.your-student-ID-number.csv.gz`\n",
    "\n",
    "### Evaluation criteria: \n",
    "  * The four tasks are equally weighted in terms of grades (25% each). \n",
    "  * With this assignment you are expected to do data preprocessing, exploratory analysis, train and evaluate machine learning models, employ (scraping and) visualisation as an analysis tool. \n",
    "  * The code cells that solve a task should follow the cell with the respective task description. Any textual analysis/description should exist in **text** cells (not in the source code) following the code cells that solve the related task. Use Jupyter's markdown-cell option to add text cells. \n",
    "  * If you borrow a solution that exists online, name the link you took it from and what you did to adapt it to your task. Detected plagiarism (esp. copying from a source without quoting and duplicate code between students) will lead to a zero grade.\n",
    "  * Your code should be well-structured and comments should explain as much as possible, to avoid misunderstandings during evaluation (points might be lost due to this).\n",
    "  * Everyone will be assessed by their written notebook, but if there are questions, some may be asked to explain in brief orally.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PDS2021-A2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
